{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm3OS3KgeK5I"
      },
      "source": [
        "# Setup env and load data\n",
        "\n",
        "This is a test project to try different computer vision models (CNN from the scratch, transfer learning etc.). As the test dataset it uses CIFAR-10:\n",
        "https://www.cs.toronto.edu/~kriz/cifar.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOS-1NZceQPz",
        "outputId": "ca59bfa1-84b7-4af4-d95c-7e4938331f26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlGjq6jly18R",
        "outputId": "41caf41a-da78-495b-c020-5186b52e3ccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua19CKhvP7yj"
      },
      "source": [
        "## Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jBYQMesrQCrf"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "path_to_google_drive_dir = Path(\"drive/MyDrive/ML-practice/computer-vision\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-qWHPcR-Y_8I"
      },
      "outputs": [],
      "source": [
        "def get_path(name: str) -> Path:\n",
        "  \"\"\"\n",
        "  Get path to the dir/file and\n",
        "  generate the parent dir if it doesn't exist.\n",
        "  \"\"\"\n",
        "  path = path_to_google_drive_dir / name\n",
        "\n",
        "  # Create dirs\n",
        "  if path.is_dir():\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "  else:\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  return path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heHPNSshQIOw"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MDH9nPYveU4w"
      },
      "outputs": [],
      "source": [
        "# Import dataset\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pb5cuT43Tv3",
        "outputId": "0d6b5123-36e5-4b81-cec4-44d8ffd69b85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(32, 32, 3)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lj2vpMmfdej",
        "outputId": "8f6ce9a6-3705-4cbf-e00d-e15d0582c6a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([6], dtype=uint8)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cBawo1bcfD7o"
      },
      "outputs": [],
      "source": [
        "# CIFAR-10 class names\n",
        "class_names = [\n",
        "    'airplane',   # 0\n",
        "    'automobile', # 1\n",
        "    'bird',       # 2\n",
        "    'cat',        # 3\n",
        "    'deer',       # 4\n",
        "    'dog',        # 5\n",
        "    'frog',       # 6\n",
        "    'horse',      # 7\n",
        "    'ship',       # 8\n",
        "    'truck',      # 9\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3uHYNvvL4-A8"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE: tuple[int, int, int] = (32, 32, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMiCUU6p49y6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-BT9GLtflA2"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w66ePUdAefqL"
      },
      "outputs": [],
      "source": [
        "def show_images(images: np.ndarray,\n",
        "                labels: np.ndarray,\n",
        "                class_names: list[str] = class_names,\n",
        "                n: int = 5) -> None:\n",
        "    \"\"\"\n",
        "    Display a row of images with their class labels.\n",
        "\n",
        "    Args:\n",
        "        images (np.array): image data (e.g., X_train)\n",
        "        labels (np.array): integer labels (e.g., y_train)\n",
        "        class_names (list): class name list for decoding\n",
        "        n (int): number of images to show\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    for i in range(n):\n",
        "        plt.subplot(1, n, i+1)\n",
        "        plt.imshow(images[i])\n",
        "        label_index = labels[i] if labels.ndim == 1 else labels[i][0]\n",
        "        plt.title(class_names[label_index])\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ordYFOXde-Wc"
      },
      "outputs": [],
      "source": [
        "# show_images(X_train, y_train, n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CUAt6lkzecRt"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Normalize pixel values\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train_cat = to_categorical(y_train, 10)\n",
        "y_test_cat = to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "7mpwYKZUfHhz",
        "outputId": "e73aca6b-6e33-4f90-e6a3-fbe13fab9667"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x300 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_images(X_train, y_train, n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX0zLwKbTzhu"
      },
      "source": [
        "#### Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MD0bq63vhRS_"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data augmentation\n",
        "def create_datagen(rotation_range: int = 15,\n",
        "                    horizontal_flip: bool = True,\n",
        "                    width_shift_range: float = 0.1,\n",
        "                    height_shift_range: float = 0.1) -> ImageDataGenerator:\n",
        "    return ImageDataGenerator(\n",
        "        rotation_range=rotation_range,\n",
        "        horizontal_flip=horizontal_flip,\n",
        "        width_shift_range=width_shift_range,\n",
        "        height_shift_range=height_shift_range\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ftPvFSoHyeJ"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-N7yhEO0HyeJ"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Union\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import Callback, History\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def load_or_train_model(create_fn: Callable[[], Model],\n",
        "                        model_path: Path,\n",
        "                        history_path: Path,\n",
        "                        callbacks: list[Callback],\n",
        "                        X_train: tf.Tensor,\n",
        "                        y_train_cat: tf.Tensor,\n",
        "                        X_test: tf.Tensor,\n",
        "                        y_test_cat: tf.Tensor,\n",
        "                        datagen: ImageDataGenerator | None = None,\n",
        "                        epochs: int = 30) -> tuple[Model, History]:\n",
        "    \"\"\"\n",
        "    Load a saved model or train a new one and save it.\n",
        "\n",
        "    Args:\n",
        "        create_fn: function that returns a compiled Keras model.\n",
        "        model_path: path to the saved model (.keras or .h5).\n",
        "        datagen: an ImageDataGenerator for data augmentation.\n",
        "        callbacks: list of Keras callbacks (e.g., EarlyStopping, ModelCheckpoint).\n",
        "        X_train, y_train_cat: training data (optional augmented using datagen).\n",
        "        X_test, y_test_cat: validation data.\n",
        "        epochs: number of training epochs.\n",
        "\n",
        "    Returns:\n",
        "        A trained or loaded Keras model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = load_model(model_path)\n",
        "        print(f\"✅ Loaded saved model from {model_path}\")\n",
        "\n",
        "        history = json.load(open(history_path))\n",
        "        print(f\"✅ Loaded history from {history_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not load model or history: {e}\")\n",
        "        model = create_fn()\n",
        "\n",
        "        if datagen:\n",
        "\n",
        "            datagen.fit(X_train)\n",
        "\n",
        "            history: History = model.fit(\n",
        "                datagen.flow(X_train, y_train_cat, batch_size=64),\n",
        "                validation_data=(X_test, y_test_cat),\n",
        "                epochs=epochs,\n",
        "                callbacks=callbacks\n",
        "            )\n",
        "        else:\n",
        "            history: History = model.fit(\n",
        "                X_train,\n",
        "                y_train_cat,\n",
        "                validation_data=(X_test, y_test_cat),\n",
        "                epochs=epochs,\n",
        "                callbacks=callbacks\n",
        "            )\n",
        "\n",
        "        model.save(model_path)\n",
        "        print(f\"💾 Model is saved to {model_path}\")\n",
        "\n",
        "        history_path.write_text(json.dumps(history.history))\n",
        "        print(f\"💾 History is saved to {history_path}\")\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3waSdTWSSz5D"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LElvAPwzCgxj"
      },
      "source": [
        "## TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "P5ubLap3CpAJ"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HZ3pcdOiCrpy"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# Create a func to build a TensorBoard callback\n",
        "\n",
        "def create_tensorboard_callback(path_to_logs: Path):\n",
        "  current_time = datetime.datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
        "  log_dir = path_to_logs / current_time\n",
        "  return tf.keras.callbacks.TensorBoard(log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-uwbhxdCoPK"
      },
      "source": [
        "## Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QJAp6-TSUd14"
      },
      "outputs": [],
      "source": [
        "# Histories dict to collect and compare the metrics\n",
        "histories: dict[str, tf.keras.callbacks.History] = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Yk-vpfYXfHh0"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(model_name: str,\n",
        "                          y_pred: np.ndarray,\n",
        "                          y_true: np.ndarray) -> None:\n",
        "    \"\"\"\n",
        "    Plot the confusion matrix of a model.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(\"Confusion Matrix for model: \" + model_name)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "c_gnrnH1S25t"
      },
      "outputs": [],
      "source": [
        "def plot_training_histories(histories: dict[str, tf.keras.callbacks.History],\n",
        "                            metric: str = 'accuracy') -> None:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for name, history in histories.items():\n",
        "        plt.plot(history.history[metric], label=f'{name} Train')\n",
        "        plt.plot(history.history[f'val_{metric}'], label=f'{name} Val')\n",
        "    plt.title(f'{metric.title()} over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(metric.title())\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFNePOp2Ibq7"
      },
      "source": [
        "#### Print general metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AAcXbqORHyeK"
      },
      "outputs": [],
      "source": [
        "def print_general_metrics(model: Model,\n",
        "                          model_name: str,\n",
        "                          model_description: str,\n",
        "                          to_print_model_summary: bool = True,\n",
        "                          to_print_classification_report: bool = True,\n",
        "                          to_plot_confusion_matrix: bool = True) -> None:\n",
        "    print(\"\\nMODEL:\", model_name)\n",
        "    print(model_description, \"\\n\")\n",
        "\n",
        "    if to_print_model_summary:\n",
        "      model.summary()\n",
        "\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    y_true = y_test.flatten()\n",
        "\n",
        "    if to_print_classification_report:\n",
        "      print(classification_report(y_true, y_pred))\n",
        "\n",
        "    if to_plot_confusion_matrix:\n",
        "      plot_confusion_matrix(model_name, y_pred, y_true)\n",
        "\n",
        "    print(\"-------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def print_accuracy_and_f1_score(model: Model,\n",
        "                                   model_name: str,\n",
        "                                   X_test: tf.Tensor = X_test,\n",
        "                                   y_test: tf.Tensor = y_test) -> None:\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    y_true = y_test.flatten()\n",
        "    print(f\"{model_name} -> Accuracy: {accuracy_score(y_true, y_pred)}, performance: {f1_score(y_true, y_pred)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import RocCurveDisplay, roc_curve, auc, roc_auc_score\n",
        "\n",
        "def plot_roc_auc_score(model: Model,\n",
        "                        model_name: str,\n",
        "                        X_test: tf.Tensor = X_test,\n",
        "                        y_test: tf.Tensor = y_test) -> None:\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    y_true = y_test.flatten()\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc).plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_roc_auc_score(model: Model,\n",
        "                        model_name: str,\n",
        "                        X_test: tf.Tensor = X_test,\n",
        "                        y_test: tf.Tensor = y_test) -> None:\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    y_true = y_test.flatten()\n",
        "    print(f\"{model_name} -> ROC AUC score: {roc_auc_score(y_true, y_pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Gr3th67EHyeK"
      },
      "outputs": [],
      "source": [
        "models_to_evaluate: list[tuple[Model, str, str]] = []  # (model, model_name, description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R0QT5VdLqcE"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSSSGDWVg6T3"
      },
      "source": [
        "## 1. CNN from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UaykaxFArRN"
      },
      "source": [
        "### Constructors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "g4vc8WDq9KMO"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class ModelConfigCCNScratch:\n",
        "    model_name: str = \"model_1_scratch_cnn\"\n",
        "    num_classes: int = len(class_names)\n",
        "    metrics: list[str] = field(default_factory=lambda: [\"accuracy\"])\n",
        "    optimizer: str = \"adam\"\n",
        "    loss_function: str = \"categorical_crossentropy\"\n",
        "    activation_function: str = \"relu\"\n",
        "    conv_layers: list[tuple[int, int, int]] = field(default_factory=lambda: [(32, 3, 3), (64, 3, 3)])\n",
        "    dense_layers: list[int] = field(default_factory=lambda: [64])\n",
        "    datagen: ImageDataGenerator | None = create_datagen()\n",
        "    epochs: int = 30\n",
        "    input_shape: tuple[int, int, int] = INPUT_SHAPE\n",
        "    create_model_fn: Callable[[], Model] | None = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZZOOw5eB9cpB"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "def train_model_cnn_scratch(\n",
        "        model_config: ModelConfigCCNScratch,\n",
        "        X_train_param: tf.Tensor = X_train,\n",
        "        y_train_param: tf.Tensor = y_train_cat,\n",
        "        X_test_param: tf.Tensor = X_test,\n",
        "        y_test_param: tf.Tensor = y_test_cat,\n",
        "        ) -> None:\n",
        "    model_description = f\"\"\"\n",
        "    Model with {len(model_config.conv_layers)} conv layers and {len(model_config.dense_layers)} dense layers.\n",
        "    Data augmentation: {'true' if model_config.datagen else 'false'}.\n",
        "    Activation function: {model_config.activation_function}.\n",
        "    Optimizer: {model_config.optimizer}.\n",
        "    Loss function: {model_config.loss_function}.\n",
        "    Metrics: {model_config.metrics}.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name = model_config.model_name\n",
        "    input_shape = model_config.input_shape\n",
        "    create_model_fn = model_config.create_model_fn\n",
        "    activation_function = model_config.activation_function\n",
        "    optimizer = model_config.optimizer\n",
        "    conv_layers = model_config.conv_layers\n",
        "    dense_layers = model_config.dense_layers\n",
        "    loss_function = model_config.loss_function\n",
        "    metrics = model_config.metrics\n",
        "    num_classes = model_config.num_classes\n",
        "    datagen = model_config.datagen\n",
        "\n",
        "    def create_model_scratch_cnn() -> Model:\n",
        "        layers = [\n",
        "            tf.keras.layers.Input(shape=input_shape),\n",
        "        ]\n",
        "\n",
        "        for conv_layer in conv_layers:\n",
        "            layers.append(tf.keras.layers.Conv2D(conv_layer[0], (conv_layer[1], conv_layer[2]), activation=activation_function))\n",
        "            layers.append(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "        layers.append(tf.keras.layers.Flatten())\n",
        "\n",
        "        for dense_layer in dense_layers:\n",
        "            layers.append(tf.keras.layers.Dense(dense_layer, activation=activation_function))\n",
        "\n",
        "        layers.append(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "        model = tf.keras.Sequential(layers)\n",
        "        model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)\n",
        "        return model\n",
        "\n",
        "    # Paths\n",
        "    path_to_model = get_path(f\"{model_name}.keras\")\n",
        "    path_to_model_history = get_path(f\"{model_name}_history.json\")\n",
        "    path_to_model_logs = get_path(f\"{model_name}_logs\")\n",
        "\n",
        "    # Train (load) model\n",
        "    trained_model, model_history = load_or_train_model(\n",
        "        create_fn=create_model_fn or create_model_scratch_cnn,\n",
        "        model_path=path_to_model,\n",
        "        history_path=path_to_model_history,\n",
        "        datagen=datagen,\n",
        "        callbacks=[\n",
        "            EarlyStopping(patience=5, restore_best_weights=True),\n",
        "            ModelCheckpoint(str(path_to_model), save_best_only=True),\n",
        "            create_tensorboard_callback(path_to_model_logs),\n",
        "        ],\n",
        "        X_train=X_train_param,\n",
        "        y_train_cat=y_train_param,\n",
        "        X_test=X_test_param,\n",
        "        y_test_cat=y_test_param\n",
        "    )\n",
        "\n",
        "    # Save history\n",
        "    histories[model_name] = model_history\n",
        "    models_to_evaluate.append((trained_model, model_name, model_description))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTFoH-V3QVjx"
      },
      "source": [
        "### 1.1. Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIKcC2zD3Elm",
        "outputId": "16c9f601-7d29-4f89-aaa0-525d9fb3ce71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded saved model from drive/MyDrive/ML-practice/computer-vision/model_1_scratch_cnn.keras\n",
            "✅ Loaded history from drive/MyDrive/ML-practice/computer-vision/model_1_scratch_cnn_history.json\n"
          ]
        }
      ],
      "source": [
        "model_config = ModelConfigCCNScratch()\n",
        "\n",
        "train_model_cnn_scratch(model_config=model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2UF46kgIEnZB"
      },
      "outputs": [],
      "source": [
        "# %tensorboard --logdir 'drive/MyDrive/ML-practice/computer-vision/model_1_scratch_cnn_logs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPMvWLlyPHhb"
      },
      "source": [
        "### 1.2. Base +1 Conv2D layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2lacF3HQBS-",
        "outputId": "e7627cf4-49cd-43b9-c8df-43857209fa3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded saved model from drive/MyDrive/ML-practice/computer-vision/model_2_scratch_cnn.keras\n",
            "✅ Loaded history from drive/MyDrive/ML-practice/computer-vision/model_2_scratch_cnn_history.json\n"
          ]
        }
      ],
      "source": [
        "model_config = ModelConfigCCNScratch(\n",
        "    model_name=\"model_2_scratch_cnn\",\n",
        "    conv_layers=[(32, 3, 3), (64, 3, 3), (128, 3, 3)],\n",
        ")\n",
        "\n",
        "train_model_cnn_scratch(model_config=model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fI7N96NPQw1Z"
      },
      "outputs": [],
      "source": [
        "# %tensorboard --logdir 'drive/MyDrive/ML-practice/computer-vision/model_2_scratch_cnn_logs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On2kVR1T1RDT"
      },
      "source": [
        "### 1.3. Base -1 Conv2D layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqMGMGcn1VaV",
        "outputId": "3231bb1d-ee2e-48df-9d01-2bf4f1803075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded saved model from drive/MyDrive/ML-practice/computer-vision/model_3_scratch_cnn.keras\n",
            "✅ Loaded history from drive/MyDrive/ML-practice/computer-vision/model_3_scratch_cnn_history.json\n"
          ]
        }
      ],
      "source": [
        "model_config = ModelConfigCCNScratch(\n",
        "    model_name=\"model_3_scratch_cnn\",\n",
        "    conv_layers=[(32, 3, 3)],\n",
        ")\n",
        "\n",
        "train_model_cnn_scratch(model_config=model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw3l99Kh1u6E"
      },
      "source": [
        "### 1.4. Base increased kernel_size (5, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmfTgAwN2Bw_",
        "outputId": "c9ebfc57-a5d0-4e7f-a8a4-9bc163ab847d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded saved model from drive/MyDrive/ML-practice/computer-vision/model_4_scratch_cnn.keras\n",
            "✅ Loaded history from drive/MyDrive/ML-practice/computer-vision/model_4_scratch_cnn_history.json\n"
          ]
        }
      ],
      "source": [
        "model_config = ModelConfigCCNScratch(\n",
        "    model_name=\"model_4_scratch_cnn\",\n",
        "    conv_layers=[(32, 5, 5), (64, 5, 5)],\n",
        ")\n",
        "\n",
        "train_model_cnn_scratch(model_config=model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0fkr9122dRS"
      },
      "source": [
        "### 1.5. Base with activation_function=\"tanh\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehPOSApT_EPx",
        "outputId": "66254778-9f34-4562-f391-bfa7988161d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded saved model from drive/MyDrive/ML-practice/computer-vision/model_5_scratch_cnn.keras\n",
            "✅ Loaded history from drive/MyDrive/ML-practice/computer-vision/model_5_scratch_cnn_history.json\n"
          ]
        }
      ],
      "source": [
        "model_config = ModelConfigCCNScratch(\n",
        "    model_name=\"model_5_scratch_cnn\",\n",
        "    activation_function=\"tanh\",\n",
        ")\n",
        "\n",
        "train_model_cnn_scratch(model_config=model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp6BzbcC_Mtt"
      },
      "source": [
        "### 1.6. Base with activation_function=\"leaky_relu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGinyvG1_NQO",
        "outputId": "59be91d6-abd6-42a7-8408-769ca3da8cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded saved model from drive/MyDrive/ML-practice/computer-vision/model_6_scratch_cnn.keras\n",
            "✅ Loaded history from drive/MyDrive/ML-practice/computer-vision/model_6_scratch_cnn_history.json\n"
          ]
        }
      ],
      "source": [
        "model_config = ModelConfigCCNScratch(\n",
        "    model_name=\"model_6_scratch_cnn\",\n",
        "    activation_function=\"leaky_relu\",\n",
        ")\n",
        "\n",
        "train_model_cnn_scratch(model_config=model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESudq92X_WY3"
      },
      "source": [
        "### 1.7. Base with optimizer=\"sgd\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG-EAq1g_iFD",
        "outputId": "0ba40a14-9e7d-40d2-cbed-3935fc3e9399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded saved model from drive/MyDrive/ML-practice/computer-vision/model_7_scratch_cnn.keras\n",
            "✅ Loaded history from drive/MyDrive/ML-practice/computer-vision/model_7_scratch_cnn_history.json\n"
          ]
        }
      ],
      "source": [
        "model_config = ModelConfigCCNScratch(\n",
        "    model_name=\"model_7_scratch_cnn\",\n",
        "    optimizer=\"sgd\",\n",
        ")\n",
        "\n",
        "train_model_cnn_scratch(model_config=model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO19BOxU_WSV"
      },
      "source": [
        "### 1.8. Base with loss_function=\"sparse_categorical_crossentropy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPgVwJjd_mLP",
        "outputId": "d89be282-b61b-4d45-85c0-37174dfbadf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded saved model from drive/MyDrive/ML-practice/computer-vision/model_8_scratch_cnn.keras\n",
            "✅ Loaded history from drive/MyDrive/ML-practice/computer-vision/model_8_scratch_cnn_history.json\n"
          ]
        }
      ],
      "source": [
        "model_config = ModelConfigCCNScratch(\n",
        "    model_name=\"model_8_scratch_cnn\",\n",
        "    loss_function=\"sparse_categorical_crossentropy\",\n",
        ")\n",
        "\n",
        "train_model_cnn_scratch(model_config=model_config, y_test_param=y_test, y_train_param=y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDaoTAMw_WP3"
      },
      "source": [
        "### 1.9. Base with dense_layers=[32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oA2FUST_mri",
        "outputId": "ffeff42e-0c7f-444c-ae29-aaca5a711fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ Could not load model or history: File not found: filepath=drive/MyDrive/ML-practice/computer-vision/model_9_scratch_cnn.keras. Please ensure the file is an accessible `.keras` zip file.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 41ms/step - accuracy: 0.3331 - loss: 1.8402 - val_accuracy: 0.5055 - val_loss: 1.3907\n",
            "Epoch 2/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 39ms/step - accuracy: 0.4959 - loss: 1.3960 - val_accuracy: 0.5890 - val_loss: 1.1688\n",
            "Epoch 3/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.5528 - loss: 1.2639 - val_accuracy: 0.6050 - val_loss: 1.1252\n",
            "Epoch 4/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.5815 - loss: 1.1836 - val_accuracy: 0.6337 - val_loss: 1.0467\n",
            "Epoch 5/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.6003 - loss: 1.1340 - val_accuracy: 0.6385 - val_loss: 1.0408\n",
            "Epoch 6/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 37ms/step - accuracy: 0.6138 - loss: 1.0947 - val_accuracy: 0.6706 - val_loss: 0.9682\n",
            "Epoch 7/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.6292 - loss: 1.0648 - val_accuracy: 0.6628 - val_loss: 0.9897\n",
            "Epoch 8/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.6373 - loss: 1.0356 - val_accuracy: 0.6638 - val_loss: 0.9796\n",
            "Epoch 9/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 38ms/step - accuracy: 0.6422 - loss: 1.0214 - val_accuracy: 0.6842 - val_loss: 0.9262\n",
            "Epoch 10/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.6545 - loss: 0.9964 - val_accuracy: 0.6954 - val_loss: 0.9045\n",
            "Epoch 11/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.6559 - loss: 0.9893 - val_accuracy: 0.6849 - val_loss: 0.9234\n",
            "Epoch 12/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.6555 - loss: 0.9789 - val_accuracy: 0.6981 - val_loss: 0.8947\n",
            "Epoch 13/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.6727 - loss: 0.9455 - val_accuracy: 0.7022 - val_loss: 0.8763\n",
            "Epoch 14/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 40ms/step - accuracy: 0.6793 - loss: 0.9256 - val_accuracy: 0.7080 - val_loss: 0.8647\n",
            "Epoch 15/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.6832 - loss: 0.9185 - val_accuracy: 0.7190 - val_loss: 0.8325\n",
            "Epoch 16/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 38ms/step - accuracy: 0.6809 - loss: 0.9156 - val_accuracy: 0.7197 - val_loss: 0.8415\n",
            "Epoch 17/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 39ms/step - accuracy: 0.6866 - loss: 0.9044 - val_accuracy: 0.7150 - val_loss: 0.8513\n",
            "Epoch 18/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - accuracy: 0.6912 - loss: 0.8889 - val_accuracy: 0.7194 - val_loss: 0.8255\n",
            "Epoch 19/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.6878 - loss: 0.8937 - val_accuracy: 0.7107 - val_loss: 0.8427\n",
            "Epoch 20/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.6943 - loss: 0.8716 - val_accuracy: 0.7192 - val_loss: 0.8255\n",
            "Epoch 21/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - accuracy: 0.7005 - loss: 0.8636 - val_accuracy: 0.7176 - val_loss: 0.8497\n",
            "Epoch 22/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 39ms/step - accuracy: 0.7006 - loss: 0.8627 - val_accuracy: 0.7080 - val_loss: 0.8671\n",
            "Epoch 23/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.7042 - loss: 0.8467 - val_accuracy: 0.7255 - val_loss: 0.8294\n",
            "💾 Model is saved to drive/MyDrive/ML-practice/computer-vision/model_9_scratch_cnn.keras\n",
            "💾 History is saved to drive/MyDrive/ML-practice/computer-vision/model_9_scratch_cnn_history.json\n"
          ]
        }
      ],
      "source": [
        "model_config = ModelConfigCCNScratch(\n",
        "    model_name=\"model_9_scratch_cnn\",\n",
        "    dense_layers=[32],\n",
        ")\n",
        "\n",
        "train_model_cnn_scratch(model_config=model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFkx-tl_WMp"
      },
      "source": [
        "### 1.10. Base with dense_layers=[128]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJEHaEbL_nWo",
        "outputId": "49ae89b3-b7e2-42b7-9d32-f36d651ddeae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ Could not load model or history: File not found: filepath=drive/MyDrive/ML-practice/computer-vision/model_10_scratch_cnn.keras. Please ensure the file is an accessible `.keras` zip file.\n",
            "Epoch 1/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 41ms/step - accuracy: 0.3548 - loss: 1.7545 - val_accuracy: 0.5244 - val_loss: 1.3509\n",
            "Epoch 2/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 37ms/step - accuracy: 0.5372 - loss: 1.3031 - val_accuracy: 0.6175 - val_loss: 1.0862\n",
            "Epoch 3/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 40ms/step - accuracy: 0.5843 - loss: 1.1735 - val_accuracy: 0.6482 - val_loss: 1.0078\n",
            "Epoch 4/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - accuracy: 0.6114 - loss: 1.0999 - val_accuracy: 0.6339 - val_loss: 1.0453\n",
            "Epoch 5/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - accuracy: 0.6336 - loss: 1.0391 - val_accuracy: 0.6654 - val_loss: 0.9469\n",
            "Epoch 6/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.6517 - loss: 0.9970 - val_accuracy: 0.6983 - val_loss: 0.8782\n",
            "Epoch 7/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.6652 - loss: 0.9469 - val_accuracy: 0.6969 - val_loss: 0.8968\n",
            "Epoch 8/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.6717 - loss: 0.9347 - val_accuracy: 0.6869 - val_loss: 0.9305\n",
            "Epoch 9/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.6852 - loss: 0.8985 - val_accuracy: 0.7181 - val_loss: 0.8242\n",
            "Epoch 10/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 39ms/step - accuracy: 0.6946 - loss: 0.8727 - val_accuracy: 0.7073 - val_loss: 0.8520\n",
            "Epoch 11/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 37ms/step - accuracy: 0.7060 - loss: 0.8380 - val_accuracy: 0.7275 - val_loss: 0.7932\n",
            "Epoch 12/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - accuracy: 0.7082 - loss: 0.8298 - val_accuracy: 0.7206 - val_loss: 0.8102\n",
            "Epoch 13/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - accuracy: 0.7124 - loss: 0.8174 - val_accuracy: 0.7317 - val_loss: 0.7861\n",
            "Epoch 14/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 37ms/step - accuracy: 0.7220 - loss: 0.7984 - val_accuracy: 0.7277 - val_loss: 0.7878\n",
            "Epoch 15/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 38ms/step - accuracy: 0.7249 - loss: 0.7856 - val_accuracy: 0.7433 - val_loss: 0.7634\n",
            "Epoch 16/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.7279 - loss: 0.7677 - val_accuracy: 0.7435 - val_loss: 0.7595\n",
            "Epoch 17/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.7355 - loss: 0.7573 - val_accuracy: 0.7527 - val_loss: 0.7469\n",
            "Epoch 18/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.7390 - loss: 0.7532 - val_accuracy: 0.7520 - val_loss: 0.7412\n",
            "Epoch 19/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.7385 - loss: 0.7431 - val_accuracy: 0.7147 - val_loss: 0.8479\n",
            "Epoch 20/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 39ms/step - accuracy: 0.7447 - loss: 0.7268 - val_accuracy: 0.7399 - val_loss: 0.7651\n",
            "Epoch 21/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.7472 - loss: 0.7230 - val_accuracy: 0.7428 - val_loss: 0.7727\n",
            "Epoch 22/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.7534 - loss: 0.7100 - val_accuracy: 0.7327 - val_loss: 0.8036\n",
            "Epoch 23/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - accuracy: 0.7580 - loss: 0.6939 - val_accuracy: 0.7592 - val_loss: 0.7215\n",
            "Epoch 24/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 37ms/step - accuracy: 0.7616 - loss: 0.6862 - val_accuracy: 0.7325 - val_loss: 0.8283\n",
            "Epoch 25/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - accuracy: 0.7581 - loss: 0.6847 - val_accuracy: 0.7663 - val_loss: 0.6983\n",
            "Epoch 26/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 39ms/step - accuracy: 0.7585 - loss: 0.6892 - val_accuracy: 0.7637 - val_loss: 0.7143\n",
            "Epoch 27/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.7674 - loss: 0.6621 - val_accuracy: 0.7494 - val_loss: 0.7570\n",
            "Epoch 28/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.7657 - loss: 0.6657 - val_accuracy: 0.7670 - val_loss: 0.7034\n",
            "Epoch 29/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 40ms/step - accuracy: 0.7679 - loss: 0.6573 - val_accuracy: 0.7664 - val_loss: 0.7139\n",
            "Epoch 30/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.7729 - loss: 0.6481 - val_accuracy: 0.7609 - val_loss: 0.7348\n",
            "💾 Model is saved to drive/MyDrive/ML-practice/computer-vision/model_10_scratch_cnn.keras\n",
            "💾 History is saved to drive/MyDrive/ML-practice/computer-vision/model_10_scratch_cnn_history.json\n"
          ]
        }
      ],
      "source": [
        "model_config = ModelConfigCCNScratch(\n",
        "    model_name=\"model_10_scratch_cnn\",\n",
        "    dense_layers=[128],\n",
        ")\n",
        "\n",
        "train_model_cnn_scratch(model_config=model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puYo1Zgt_V-U"
      },
      "source": [
        "### 1.11. Base without datagen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swBCiehE_nyl",
        "outputId": "b5ddb1f7-1448-4014-fe92-764e80aa9efa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ Could not load model or history: File not found: filepath=drive/MyDrive/ML-practice/computer-vision/model_11_scratch_cnn.keras. Please ensure the file is an accessible `.keras` zip file.\n",
            "Epoch 1/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.3875 - loss: 1.6826 - val_accuracy: 0.5625 - val_loss: 1.2233\n",
            "Epoch 2/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6033 - loss: 1.1325 - val_accuracy: 0.6265 - val_loss: 1.0662\n",
            "Epoch 3/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.6606 - loss: 0.9835 - val_accuracy: 0.6607 - val_loss: 0.9880\n",
            "Epoch 4/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.6888 - loss: 0.9031 - val_accuracy: 0.6789 - val_loss: 0.9340\n",
            "Epoch 5/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7141 - loss: 0.8268 - val_accuracy: 0.6860 - val_loss: 0.9118\n",
            "Epoch 6/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7378 - loss: 0.7633 - val_accuracy: 0.6865 - val_loss: 0.9275\n",
            "Epoch 7/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7503 - loss: 0.7148 - val_accuracy: 0.6812 - val_loss: 0.9398\n",
            "Epoch 8/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7676 - loss: 0.6683 - val_accuracy: 0.6877 - val_loss: 0.9260\n",
            "Epoch 9/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7831 - loss: 0.6249 - val_accuracy: 0.6934 - val_loss: 0.9268\n",
            "Epoch 10/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7947 - loss: 0.5866 - val_accuracy: 0.7003 - val_loss: 0.9088\n",
            "Epoch 11/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8082 - loss: 0.5517 - val_accuracy: 0.7061 - val_loss: 0.9231\n",
            "Epoch 12/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8140 - loss: 0.5269 - val_accuracy: 0.6980 - val_loss: 0.9823\n",
            "Epoch 13/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8299 - loss: 0.4901 - val_accuracy: 0.6932 - val_loss: 1.0076\n",
            "Epoch 14/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8383 - loss: 0.4617 - val_accuracy: 0.6953 - val_loss: 1.0246\n",
            "Epoch 15/30\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.8502 - loss: 0.4244 - val_accuracy: 0.6977 - val_loss: 1.0244\n",
            "💾 Model is saved to drive/MyDrive/ML-practice/computer-vision/model_11_scratch_cnn.keras\n",
            "💾 History is saved to drive/MyDrive/ML-practice/computer-vision/model_11_scratch_cnn_history.json\n"
          ]
        }
      ],
      "source": [
        "model_config = ModelConfigCCNScratch(\n",
        "    model_name=\"model_11_scratch_cnn\",\n",
        "    datagen=None,\n",
        ")\n",
        "\n",
        "train_model_cnn_scratch(model_config=model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ha_5SpK_Vqb"
      },
      "source": [
        "### 1.12. Base with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5dYC-phQ_VdC"
      },
      "outputs": [],
      "source": [
        "# model_config = ModelConfigCCNScratch(\n",
        "#     model_name=\"model_12_scratch_cnn\",\n",
        "# )\n",
        "\n",
        "# train_model_cnn_scratch(model_config=model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uiYwVEj0R2B"
      },
      "source": [
        "### 1.13. Base with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "6LWuharzU9yC"
      },
      "outputs": [],
      "source": [
        "# model_config = ModelConfigCCNScratch(\n",
        "#     model_name=\"model_13_scratch_cnn\",\n",
        "#     conv_layers=[(32, 5, 5), (64, 5, 5)],\n",
        "# )\n",
        "\n",
        "# train_model_cnn_scratch(model_config=model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiGkeRMMHyeK"
      },
      "source": [
        "# Evaluate all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiIwDOziHyeK",
        "outputId": "6456d8d8-c48f-49ce-ef97-bf0b635b2d7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MODEL: model_1_scratch_cnn\n",
            "\n",
            "    Model with 2 conv layers and 1 dense layers.\n",
            "    Data augmentation: true.\n",
            "    Activation function: relu.\n",
            "    Optimizer: adam.\n",
            "    Loss function: categorical_crossentropy.\n",
            "    Metrics: ['accuracy'].\n",
            "     \n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.75      0.75      1000\n",
            "           1       0.87      0.83      0.85      1000\n",
            "           2       0.70      0.53      0.60      1000\n",
            "           3       0.56      0.48      0.52      1000\n",
            "           4       0.66      0.68      0.67      1000\n",
            "           5       0.64      0.62      0.63      1000\n",
            "           6       0.69      0.86      0.77      1000\n",
            "           7       0.72      0.81      0.76      1000\n",
            "           8       0.82      0.82      0.82      1000\n",
            "           9       0.79      0.83      0.81      1000\n",
            "\n",
            "    accuracy                           0.72     10000\n",
            "   macro avg       0.72      0.72      0.72     10000\n",
            "weighted avg       0.72      0.72      0.72     10000\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\n",
            "MODEL: model_2_scratch_cnn\n",
            "\n",
            "    Model with 3 conv layers and 1 dense layers.\n",
            "    Data augmentation: true.\n",
            "    Activation function: relu.\n",
            "    Optimizer: adam.\n",
            "    Loss function: categorical_crossentropy.\n",
            "    Metrics: ['accuracy'].\n",
            "     \n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.77      0.78      1000\n",
            "           1       0.84      0.88      0.86      1000\n",
            "           2       0.70      0.65      0.67      1000\n",
            "           3       0.67      0.51      0.58      1000\n",
            "           4       0.78      0.64      0.70      1000\n",
            "           5       0.73      0.63      0.68      1000\n",
            "           6       0.77      0.87      0.82      1000\n",
            "           7       0.66      0.88      0.75      1000\n",
            "           8       0.83      0.86      0.85      1000\n",
            "           9       0.77      0.84      0.80      1000\n",
            "\n",
            "    accuracy                           0.75     10000\n",
            "   macro avg       0.75      0.75      0.75     10000\n",
            "weighted avg       0.75      0.75      0.75     10000\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\n",
            "MODEL: model_3_scratch_cnn\n",
            "\n",
            "    Model with 1 conv layers and 1 dense layers.\n",
            "    Data augmentation: true.\n",
            "    Activation function: relu.\n",
            "    Optimizer: adam.\n",
            "    Loss function: categorical_crossentropy.\n",
            "    Metrics: ['accuracy'].\n",
            "     \n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.63      0.67      1000\n",
            "           1       0.74      0.77      0.76      1000\n",
            "           2       0.62      0.38      0.48      1000\n",
            "           3       0.55      0.30      0.39      1000\n",
            "           4       0.51      0.69      0.58      1000\n",
            "           5       0.66      0.44      0.53      1000\n",
            "           6       0.59      0.81      0.68      1000\n",
            "           7       0.67      0.73      0.70      1000\n",
            "           8       0.71      0.77      0.74      1000\n",
            "           9       0.59      0.80      0.68      1000\n",
            "\n",
            "    accuracy                           0.63     10000\n",
            "   macro avg       0.64      0.63      0.62     10000\n",
            "weighted avg       0.64      0.63      0.62     10000\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\n",
            "MODEL: model_4_scratch_cnn\n",
            "\n",
            "    Model with 2 conv layers and 1 dense layers.\n",
            "    Data augmentation: true.\n",
            "    Activation function: relu.\n",
            "    Optimizer: adam.\n",
            "    Loss function: categorical_crossentropy.\n",
            "    Metrics: ['accuracy'].\n",
            "     \n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.78      0.76      1000\n",
            "           1       0.83      0.87      0.85      1000\n",
            "           2       0.77      0.45      0.57      1000\n",
            "           3       0.52      0.55      0.54      1000\n",
            "           4       0.71      0.59      0.65      1000\n",
            "           5       0.70      0.55      0.62      1000\n",
            "           6       0.66      0.87      0.75      1000\n",
            "           7       0.72      0.80      0.76      1000\n",
            "           8       0.81      0.87      0.84      1000\n",
            "           9       0.76      0.84      0.80      1000\n",
            "\n",
            "    accuracy                           0.72     10000\n",
            "   macro avg       0.72      0.72      0.71     10000\n",
            "weighted avg       0.72      0.72      0.71     10000\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\n",
            "MODEL: model_5_scratch_cnn\n",
            "\n",
            "    Model with 2 conv layers and 1 dense layers.\n",
            "    Data augmentation: true.\n",
            "    Activation function: tanh.\n",
            "    Optimizer: adam.\n",
            "    Loss function: categorical_crossentropy.\n",
            "    Metrics: ['accuracy'].\n",
            "     \n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.79      0.73      1000\n",
            "           1       0.77      0.87      0.82      1000\n",
            "           2       0.67      0.56      0.61      1000\n",
            "           3       0.66      0.40      0.50      1000\n",
            "           4       0.73      0.57      0.64      1000\n",
            "           5       0.67      0.58      0.62      1000\n",
            "           6       0.69      0.85      0.76      1000\n",
            "           7       0.63      0.86      0.73      1000\n",
            "           8       0.83      0.79      0.81      1000\n",
            "           9       0.77      0.80      0.78      1000\n",
            "\n",
            "    accuracy                           0.71     10000\n",
            "   macro avg       0.71      0.71      0.70     10000\n",
            "weighted avg       0.71      0.71      0.70     10000\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\n",
            "MODEL: model_6_scratch_cnn\n",
            "\n",
            "    Model with 2 conv layers and 1 dense layers.\n",
            "    Data augmentation: true.\n",
            "    Activation function: leaky_relu.\n",
            "    Optimizer: adam.\n",
            "    Loss function: categorical_crossentropy.\n",
            "    Metrics: ['accuracy'].\n",
            "     \n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.86      0.78      1000\n",
            "           1       0.89      0.87      0.88      1000\n",
            "           2       0.75      0.59      0.66      1000\n",
            "           3       0.66      0.53      0.59      1000\n",
            "           4       0.70      0.73      0.72      1000\n",
            "           5       0.74      0.66      0.70      1000\n",
            "           6       0.81      0.84      0.83      1000\n",
            "           7       0.73      0.85      0.78      1000\n",
            "           8       0.90      0.77      0.83      1000\n",
            "           9       0.74      0.91      0.82      1000\n",
            "\n",
            "    accuracy                           0.76     10000\n",
            "   macro avg       0.76      0.76      0.76     10000\n",
            "weighted avg       0.76      0.76      0.76     10000\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\n",
            "MODEL: model_7_scratch_cnn\n",
            "\n",
            "    Model with 2 conv layers and 1 dense layers.\n",
            "    Data augmentation: true.\n",
            "    Activation function: relu.\n",
            "    Optimizer: sgd.\n",
            "    Loss function: categorical_crossentropy.\n",
            "    Metrics: ['accuracy'].\n",
            "     \n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.76      0.72      1000\n",
            "           1       0.68      0.87      0.77      1000\n",
            "           2       0.69      0.47      0.55      1000\n",
            "           3       0.54      0.49      0.51      1000\n",
            "           4       0.69      0.56      0.62      1000\n",
            "           5       0.59      0.60      0.60      1000\n",
            "           6       0.75      0.74      0.75      1000\n",
            "           7       0.68      0.77      0.72      1000\n",
            "           8       0.81      0.75      0.78      1000\n",
            "           9       0.66      0.76      0.71      1000\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.68      0.68      0.67     10000\n",
            "weighted avg       0.68      0.68      0.67     10000\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\n",
            "MODEL: model_8_scratch_cnn\n",
            "\n",
            "    Model with 2 conv layers and 1 dense layers.\n",
            "    Data augmentation: true.\n",
            "    Activation function: relu.\n",
            "    Optimizer: adam.\n",
            "    Loss function: sparse_categorical_crossentropy.\n",
            "    Metrics: ['accuracy'].\n",
            "     \n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.82      0.77      1000\n",
            "           1       0.86      0.84      0.85      1000\n",
            "           2       0.68      0.60      0.63      1000\n",
            "           3       0.58      0.55      0.56      1000\n",
            "           4       0.75      0.59      0.66      1000\n",
            "           5       0.65      0.66      0.66      1000\n",
            "           6       0.76      0.84      0.80      1000\n",
            "           7       0.70      0.83      0.76      1000\n",
            "           8       0.89      0.77      0.83      1000\n",
            "           9       0.78      0.84      0.81      1000\n",
            "\n",
            "    accuracy                           0.73     10000\n",
            "   macro avg       0.74      0.73      0.73     10000\n",
            "weighted avg       0.74      0.73      0.73     10000\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\n",
            "MODEL: model_9_scratch_cnn\n",
            "\n",
            "    Model with 2 conv layers and 1 dense layers.\n",
            "    Data augmentation: true.\n",
            "    Activation function: relu.\n",
            "    Optimizer: adam.\n",
            "    Loss function: categorical_crossentropy.\n",
            "    Metrics: ['accuracy'].\n",
            "     \n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.81      0.76      1000\n",
            "           1       0.77      0.89      0.82      1000\n",
            "           2       0.68      0.56      0.61      1000\n",
            "           3       0.63      0.50      0.56      1000\n",
            "           4       0.74      0.57      0.64      1000\n",
            "           5       0.68      0.64      0.65      1000\n",
            "           6       0.72      0.84      0.77      1000\n",
            "           7       0.66      0.84      0.74      1000\n",
            "           8       0.87      0.74      0.80      1000\n",
            "           9       0.75      0.82      0.78      1000\n",
            "\n",
            "    accuracy                           0.72     10000\n",
            "   macro avg       0.72      0.72      0.71     10000\n",
            "weighted avg       0.72      0.72      0.71     10000\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\n",
            "MODEL: model_10_scratch_cnn\n",
            "\n",
            "    Model with 2 conv layers and 1 dense layers.\n",
            "    Data augmentation: true.\n",
            "    Activation function: relu.\n",
            "    Optimizer: adam.\n",
            "    Loss function: categorical_crossentropy.\n",
            "    Metrics: ['accuracy'].\n",
            "     \n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.77      0.80      1000\n",
            "           1       0.87      0.86      0.87      1000\n",
            "           2       0.79      0.60      0.68      1000\n",
            "           3       0.65      0.55      0.60      1000\n",
            "           4       0.73      0.74      0.73      1000\n",
            "           5       0.67      0.70      0.69      1000\n",
            "           6       0.78      0.85      0.81      1000\n",
            "           7       0.79      0.84      0.82      1000\n",
            "           8       0.86      0.84      0.85      1000\n",
            "           9       0.71      0.90      0.80      1000\n",
            "\n",
            "    accuracy                           0.77     10000\n",
            "   macro avg       0.77      0.77      0.76     10000\n",
            "weighted avg       0.77      0.77      0.76     10000\n",
            "\n",
            "-------------------------------------------------------------------\n",
            "\n",
            "MODEL: model_11_scratch_cnn\n",
            "\n",
            "    Model with 2 conv layers and 1 dense layers.\n",
            "    Data augmentation: false.\n",
            "    Activation function: relu.\n",
            "    Optimizer: adam.\n",
            "    Loss function: categorical_crossentropy.\n",
            "    Metrics: ['accuracy'].\n",
            "     \n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.74      0.73      1000\n",
            "           1       0.78      0.85      0.81      1000\n",
            "           2       0.56      0.60      0.58      1000\n",
            "           3       0.57      0.48      0.52      1000\n",
            "           4       0.63      0.70      0.67      1000\n",
            "           5       0.62      0.61      0.61      1000\n",
            "           6       0.80      0.74      0.77      1000\n",
            "           7       0.81      0.67      0.73      1000\n",
            "           8       0.77      0.82      0.79      1000\n",
            "           9       0.76      0.79      0.77      1000\n",
            "\n",
            "    accuracy                           0.70     10000\n",
            "   macro avg       0.70      0.70      0.70     10000\n",
            "weighted avg       0.70      0.70      0.70     10000\n",
            "\n",
            "-------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for model, model_name, model_description in models_to_evaluate:\n",
        "    print_general_metrics(\n",
        "        model=model,\n",
        "        model_name=model_name,\n",
        "        model_description=model_description,\n",
        "        to_print_model_summary = False,\n",
        "        to_print_classification_report = True,\n",
        "        to_plot_confusion_matrix = False,\n",
        "      )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Wo10wCyfJN3s"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv-ds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
